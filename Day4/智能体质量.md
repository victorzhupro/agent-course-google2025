智能体质量

作者：Meltem Subasioglu, Turan Bulmus, Wafae Bakkali

# **致谢**

## **内容贡献者**

· Hussain Chinoy

· Ale Fin

· Peter Grabowski

· Michelle Liu

· Anant Nawalgaria

· Kanchana Patlolla

· Steven Pecht

· Julia Wiesinger

## **策展人和编辑**

· Anant Nawalgaria

· Kanchana Patlolla

## **设计师**

· Michael Lanning

# **目录**

· 引言

· 如何阅读本白皮书

1. 2. 非确定性世界中的智能体质量
2. 2.1 为什么智能体质量需要新方法
3. 2.2 范式转变：从可预测的代码到不可预测的智能体
4. 2.3 智能体质量的支柱：评估框架
5. 2.4 总结与展望
6. 3. 智能体评估的艺术：判断过程
7. 3.1 战略框架："由外向内"评估层次结构
8. 3.2 "由外向内"视图：端到端评估（黑盒）
9. 3.3 "由内向外"视图：轨迹评估（玻璃盒）
10. 3.4 评估者：智能体判断的主体与对象
11. 3.5 自动化指标
12. 3.6 LLM作为评判者范式
13. 3.7 智能体作为评判者
14. 3.8 人在环（HITL）评估
15. 3.9 用户反馈与审阅者界面
16. 3.10 超越性能：负责任AI（RAI）与安全评估
17. 3.11 总结与展望
18. 4. 可观测性：洞察智能体的思维
19. 4.1 从监控到真正的可观测性
20. 4.2 厨房类比：流水线厨师与美食主厨
21. 4.3 可观测性的三大支柱
22. 4.4 支柱1：日志记录——智能体的日记
23. 4.5 支柱2：追踪——跟随智能体的脚步
24. 4.6 为什么追踪不可或缺
25. 4.7 智能体追踪的关键要素
26. 4.8 支柱3：指标——智能体的健康报告
27. 4.9 系统指标：生命体征
28. 4.10 质量指标：评判决策制定
29. 4.11 整合一切：从原始数据到可操作的洞察
30. 4.12 总结与展望
31. 5. 结论：在自主世界中建立信任
32. 5.1 引言：从自主能力到企业信任
33. 5.2 智能体质量飞轮：框架综合
34. 5.3 构建可信智能体的三大核心原则
35. 5.4 未来是智能体的——也是可靠的

· 参考文献

# **1. 引言**

我们正处于智能体时代的黎明。从可预测的、基于指令的工具向自主的、目标导向的AI智能体的转变，是几十年来软件工程中最深刻的变革之一。虽然这些智能体释放了令人难以置信的能力，但它们固有的非确定性使它们变得不可预测，并粉碎了我们传统的质量保证模式。

本白皮书是应对这一新现实的实用指南，基于一个简单但激进的原则：

智能体质量是架构支柱，而不是最终的测试阶段。

本指南建立在三个核心信息之上：

• 轨迹即真理：我们必须超越仅评估最终输出的范畴。智能体质量和安全性的真正衡量标准在于其整个决策过程。

• 可观测性是基础：你无法判断一个你看不见的过程。我们详细介绍了可观测性的"三大支柱"——日志记录、追踪和指标——作为捕获智能体"思维过程"的基本技术基础。

• 评估是一个持续的循环：我们将这些概念综合为"智能体质量飞轮"，一个将这些数据转化为可操作洞察力的运营手册。该系统使用可扩展的AI驱动评估器和不可或缺的人机回环（HITL）判断来推动持续的改进。

本白皮书面向构建未来的架构师、工程师和产品负责人。它提供了从构建有能力的智能体到构建可靠和值得信赖的智能体的框架。

## **1.1 如何阅读本白皮书**

本指南的结构从"为什么"到"是什么"，最后到"如何做"。使用本节导航到与您角色最相关的章节。

面向所有读者：从第1章"非确定性世界中的智能体质量"开始。本章阐述了核心问题，解释了传统QA为什么在AI智能体上失败，并介绍了定义我们目标的智能体质量的四大支柱（有效性、效率、鲁棒性和安全性）。

面向产品经理、数据科学家和QA领导者：如果您负责衡量什么以及如何评判质量，请重点关注第2章"智能体评估的艺术"。本章是您的战略指南，详细介绍了评估的"由外向内"层次结构，解释了可扩展的"LLM作为评判者"范式，并阐明了人在环（HITL）评估的关键作用。

面向工程师、架构师和SRE：如果您构建系统，您的技术蓝图是第3章"可观测性"。本章从理论转向实施，用"厨房类比"（流水线厨师与美食主厨）来解释监控与可观测性，并详细介绍了可观测性的三大支柱：日志、追踪和指标——这些是构建"可评估"智能体所需的工具。

面向团队负责人和战略家：要了解这些部分如何创建一个自我改进的系统，请阅读第4章"结论"。本章将这些概念统一到一个运营手册中，介绍了"智能体质量飞轮"作为持续改进的模型，并总结了构建可信AI的三个核心原则。

# **2. 非确定性世界中的智能体质量**

人工智能世界正在全速转型。我们正在从构建执行指令的可预测工具转向设计能够解释意图、制定计划并执行复杂多步动作的智能体。对于构建、竞争和部署尖端技术的数据科学家和工程师来说，这一转变带来了深刻的挑战。使AI智能体强大的机制也使它们变得不可预测。

为了理解这种转变，将传统软件比作送货卡车，将AI智能体比作F1赛车。卡车只需要基本检查（"发动机启动了吗？它遵循了固定路线吗？"）。像AI智能体一样的赛车是一个复杂的自主系统，其成功取决于动态判断。它的评估不能是一个简单的清单；它需要连续的遥测来判断每个决策的质量——从燃油消耗到刹车策略。

这种演变正在根本性地改变我们必须如何处理软件质量。传统的质量保证（QA）实践虽然对确定性系统很强大，但对于现代AI的细微和涌现行为来说是不够的。智能体可能通过100个单元测试，但在生产环境中仍然灾难性地失败，因为它的失败不是代码中的错误；而是判断上的缺陷。

传统软件验证问："我们构建的产品正确吗？"它将逻辑与固定规范进行验证。现代AI评估必须问一个更复杂的问题："我们构建了正确的产品吗？"这是一个验证过程，在动态和不确定的世界中评估质量、鲁棒性和可信度。

本章检视这一新范式。我们将探讨为什么智能体质量需要新方法，分析使我们的旧方法过时的技术转变，并建立战略性的"由外向内"框架来评估"思考"的系统。

## **2.1 为什么智能体质量需要新方法**

对于工程师来说，风险是需要识别和缓解的。在传统软件中，失败是明确的：系统崩溃、抛出NullPointerException或返回明显错误的计算结果。这些失败是明显的、确定性的，可以追溯到逻辑中的特定错误。

AI智能体的失败方式不同。它们的失败通常不是系统崩溃，而是质量的细微下降，源于模型权重、训练数据和环境交互之间的复杂相互作用。这些失败是阴险的：系统继续运行，API调用返回200 OK，输出看起来合理。但它 profoundly wrong、操作上危险，并默默地侵蚀信任。

未能掌握这种转变的组织面临重大失败、运营效率低下和声誉损害。虽然算法偏见和概念漂移等失败模式在被动模型中已经存在，但智能体的自主性和复杂性加剧了这些风险，使它们更难追踪和缓解。

这些失败使传统的调试和测试范式失效。你无法使用断点来调试幻觉。你无法编写单元测试来防止涌现偏见。根本原因分析需要深度数据分析、模型重新训练和系统评估——这是一个全新的学科。

## **2.2 范式转变：从可预测的代码到不可预测的智能体**

核心技术挑战源于从以模型为中心的AI到以系统为中心的AI的演变。评估AI智能体与评估算法根本不同，因为智能体是一个系统。这种演变以复合阶段发生，每个阶段都增加了新的评估复杂性。

阶段1：传统机器学习
评估回归或分类模型虽然不平凡，但这是一个定义明确的问题。我们依赖统计指标，如精确率、召回率、F1分数和针对保留测试集的RMSE。问题很复杂，但"正确"的定义是明确的。

阶段2：被动LLM
随着生成模型的兴起，我们失去了简单的指标。我们如何衡量生成段落的"准确性"？输出是概率性的。即使输入相同，输出也可能不同。评估变得更加复杂，依赖人工评分者和模型对比基准测试。尽管如此，这些系统在很大程度上还是被动的、文本输入、文本输出的工具。

阶段3：LLM+RAG（检索增强生成）
下一个飞跃引入了多组件管道。现在，失败可能发生在LLM或检索系统中。智能体给出坏答案是因为LLM推理能力差，还是因为向量数据库检索到不相关的片段？我们的评估表面从仅模型扩展到包括分块策略、嵌入和检索器的性能。

阶段4：主动AI智能体
今天，我们面临着深刻的架构转变。LLM不再只是文本生成器；它是一个复杂系统中的推理"大脑"，集成到能够自主行动的循环中。这种智能体系统引入了三个打破我们评估模型的核心技术能力：

• 规划和多步推理：智能体将复杂目标（"规划我的旅行"）分解为多个子任务。这创建了一个轨迹（思考→行动→观察→思考...）。LLM的非确定性现在在每一步都复合。第一步中一个小的随机词选择可能使智能体在第四步走上完全不同且无法恢复的路径。

• 工具使用和函数调用：智能体通过API和外部工具（代码解释器、搜索引擎、预订API）与现实世界交互。这引入了动态环境交互。智能体的下一步行动完全取决于外部、不可控世界的状态。

• 记忆：智能体维护状态。短期"草稿"记忆跟踪当前任务，而长期记忆允许智能体从过去的交互中学习。这意味着智能体的行为会演变，昨天有效的输入可能因为智能体"学到"的内容而今天产生不同的结果。

阶段5：多智能体系统
最终的架构复杂性来自于将多个主动智能体集成到共享环境中。这不再是对单个轨迹的评估，而是对系统级涌现现象的评估，引入了新的根本性挑战：

• 涌现系统失败：系统的成功取决于智能体之间无脚本的交互，如资源争用、通信瓶颈和系统性死锁，这些都不能归因于单个智能体的失败。

• 合作与竞争评估：目标函数本身可能变得模糊。在合作多智能体系统（如供应链优化）中，成功是一个全局指标，而在竞争多智能体系统（如博弈论场景或拍卖系统）中，评估通常需要跟踪单个智能体性能以及整体市场/环境的稳定性。

这些能力的结合意味着评估的主要单位不再是模型，而是整个系统轨迹。智能体的涌现行为源于其规划模块、工具、记忆和动态环境之间的复杂相互作用。

## **2.3 智能体质量的支柱：评估框架**

如果我们不能再依赖简单的准确性指标，而且必须评估整个系统，我们从哪里开始？答案是一种被称为"由外向内"方法的战略转变。

这种方法将AI评估锚定在以用户为中心的指标和总体业务目标上，超越仅依赖内部、组件级技术评分的做法。我们必须停止只问"模型的F1分数是多少？"，而开始问"这个智能体是否提供了可衡量的价值并符合用户的意图？"

这种策略需要一个将高级业务目标与技术性能联系起来的整体框架。我们在四个相互关联的支柱上定义智能体质量：

有效性（目标达成）：这是最终的"黑盒"问题：智能体是否成功且准确地达成了用户的实际意图？这个支柱直接与以用户为中心的指标和业务KPI相关联。对于零售智能体，这不只是"它是否找到了产品？"，而是"它是否推动了转化？"对于数据分析智能体，不是"它是否编写了代码？"，而是"代码是否产生了正确的洞察？"有效性是任务成功的最终衡量标准。

效率（运营成本）：智能体是否很好地解决了问题？一个花费25步、5次失败的工具调用和3个自我纠正循环来预订简单航班的智能体可以被认为是低质量的——即使它最终成功了。效率以消耗的资源衡量：总token数（成本）、挂钟时间（延迟）和轨迹复杂性（总步数）。

鲁棒性（可靠性）：智能体如何处理逆境和现实世界的混乱？当API超时、网站布局改变、数据缺失或用户提供模糊提示时，智能体是否优雅地失败？一个有韧性的智能体会重试失败的调用，在需要时向用户询问澄清，并报告它无法做什么以及为什么，而不是崩溃或产生幻觉。

安全性与对齐（可信度）：这是不可协商的门槛。智能体是否在其定义的道德边界和约束内运作？这个支柱涵盖了从公平性和偏见的负责任AI指标到防止提示注入和数据泄露的安全性。它确保智能体保持任务状态，拒绝有害指令，并作为您组织的可信代理运作。

这个框架明确了一件事：如果你只看到最终答案，你就无法衡量这些支柱中的任何一个。如果你不计数步数，你就无法衡量效率。如果你不知道哪个API调用失败，你就无法诊断鲁棒性失败。如果你无法检查智能体的内部推理，你就无法验证安全性。

智能体质量的整体框架要求智能体可见性的整体架构。

## **2.4 总结与展望**

智能体固有的非确定性性质已经打破了传统的质量保证。风险现在包括偏见、幻觉和漂移等细微问题，这是由从被动模型到规划和工具使用的主动、以系统为中心的智能体的转变所驱动的。我们必须将重点从验证（检查规范）转移到验证（评判价值）。

这需要一种衡量智能体质量的"由外向内"框架，涵盖四大支柱：有效性、效率、鲁棒性和安全性。衡量这些支柱需要深度可见性——看到智能体的决策制定轨迹内部。

在构建"如何"（可观测性架构）之前，我们必须定义"什么"：好的评估是什么样的？

第2章将定义评估复杂智能体行为的策略和评判者。
第3章将构建捕获数据所需的技术基础（日志记录、追踪和指标）。

# **3. 智能体评估的艺术：判断过程**

在第1章中，我们确立了从传统软件测试到现代AI评估的根本转变。传统测试是一个确定性的验证过程——它问"我们构建的产品正确吗？"对照固定规范。这种方法在系统的核心逻辑是概率性的时失败，因为非确定性输出更可能引入不会导致明确崩溃且可能不可重复的细微质量下降。

相比之下，智能体评估是一个整体的验证过程。它问一个更复杂和关键的战略问题："我们构建了正确的产品吗？"这个问题是"由外向内"评估框架的战略锚点，代表着从内部合规到评判系统外部价值和与用户意图对齐的必要转变。这要求我们在动态世界中评估智能体的整体质量、鲁棒性和用户价值。

能够规划、使用工具并与复杂环境交互的AI智能体的兴起，极大地复杂化了这一评估格局。我们必须超越"测试"输出，学习"评估"过程的艺术。本章为此提供了战略框架：评判智能体从初始意图到最终结果的整个决策轨迹。

## **3.1 战略框架："由外向内"评估层次结构**

为了避免迷失在组件级指标的海洋中，评估必须是一个自上而下的战略过程。我们称之为"由外向内"层次结构。这种方法优先考虑唯一最终重要的指标——现实世界的成功——然后再深入探讨这种成功发生或不发生的技术细节。这个模型是一个两阶段过程：从黑盒开始，然后打开它。

## **3.2 "由外向内"视图：端到端评估（黑盒）**

第一个也是最重要的问题是："智能体是否有效地达成了用户的目标？"

这是"由外向内"视图。在分析任何内部思考或工具调用之前，我们必须根据定义的目标评估智能体的最终性能。

这一阶段的指标侧重于整体任务完成。我们衡量：

• 任务成功率：最终输出是否正确、完整并解决了用户实际问题的二元（或分级）评分，例如编码智能体的PR接受率、金融智能体的成功数据库事务率或客服机器人的会话完成率。

• 用户满意度：对于交互式智能体，这可以是直接的用户反馈评分（如赞/踩）或客户满意度评分（CSAT）。

• 整体质量：如果智能体的目标是定量的（如"总结这10篇文章"），指标可能是准确性或完整性（如"它是否总结了全部10篇？"）。

如果智能体在这一阶段得了100%的分数，我们的工作可能就完成了。但在复杂系统中，这种情况很少见。当智能体产生有缺陷的最终输出、放弃任务或无法收敛到解决方案时，"由外向内"视图告诉我们什么出错了。现在我们必须打开盒子看看为什么。

## **3.3 "由内向外"视图：轨迹评估（玻璃盒）**

一旦发现失败，我们就转向"由内向外"视图。我们通过系统评估其执行轨迹的每个组件来分析智能体的方法：

1. LLM规划（"思考"）：我们首先检查核心推理。LLM本身有问题吗？这里的失败包括幻觉、无意义或离题的回答、上下文污染或重复的输出循环。
2. 工具使用（选择与参数化）：智能体的好坏取决于它的工具。我们必须分析智能体是否调用了错误的工具、未能调用必要的工具、产生工具名或参数名/类型的幻觉，或不必要的调用。即使它选择了正确的工具，也可能因提供缺失的参数、错误的数据类型或格式错误的JSON API调用而失败。
3. 工具响应解释（"观察"）：工具正确执行后，智能体必须理解结果。智能体经常在这里失败，误解数值数据、未能从响应中提取关键实体，或关键的是，没有识别工具返回的错误状态（如API的404错误）并继续调用成功。
4. RAG性能：如果智能体使用检索增强生成（RAG），轨迹取决于其检索信息的质量。失败包括不相关的文档检索、获取过时或不正确的信息，或LLM完全忽略检索的上下文并仍然产生幻觉答案。
5. 轨迹效率和鲁棒性：除了正确性，我们还必须评估过程本身：揭示低效的资源分配，如过多的API调用、高延迟或重复努力。它还揭示了鲁棒性失败，如未处理的异常。
6. 多智能体动态：在高级系统中，轨迹涉及多个智能体。评估还必须包括智能体间通信日志，以检查误解或通信循环，并确保智能体遵守其定义的角色而不与其他智能体冲突。

通过分析追踪，我们可以从"最终答案是错误的"（黑盒）转向"最终答案是错误的，因为……"（玻璃盒）。这种诊断能力是整个智能体评估的目标。

## **3.4 评估者：智能体判断的主体与对象**

知道要评估什么（轨迹）只是战斗的一半。另一半是如何评判它。对于质量、安全性和可解释性等细微方面，这种判断需要一种复杂的混合方法。自动化系统提供规模，但人类判断仍然是质量的关键仲裁者。

## **3.5 自动化指标**

自动化指标提供速度和可重复性。它们对回归测试和输出基准测试很有用。示例包括：

• 基于字符串的相似性（ROUGE、BLEU），将生成的文本与参考文本进行比较。

• 基于嵌入的相似性（BERTScore、余弦相似性），衡量语义接近度。

• 任务特定的基准，例如TruthfulQA

指标高效但浅显：它们捕捉表面相似性，而不是深层推理或用户价值。

## **3.6 LLM作为评判者范式**

我们如何自动化评估定性输出，如"这个摘要好吗？"或"这个计划合乎逻辑吗？"答案是用我们试图评估的同样技术。LLM作为评判者范式涉及使用一个强大的、最先进的模型（如Google的Gemini Advanced）来评估另一个智能体的输出。

我们为"评判者"LLM提供智能体的输出、原始提示、"黄金"答案或参考（如果存在），以及详细的评估标准（例如，"在1-5的尺度上评估这个回答的有用性、正确性和安全性，并解释你的推理。"）

这种方法提供可扩展、快速且令人惊讶的细微反馈，特别是对于中间步骤，如智能体"思考"的质量或其工具响应的解释。虽然它不能取代人类判断，但它允许数据科学团队快速评估数千个场景的性能，使迭代评估过程可行。

## **3.7 智能体作为评判者**

虽然LLM可以对最终响应进行评分，但智能体需要对推理和行动进行更深入的评估。新兴的智能体作为评判者范式使用一个智能体来评估另一个智能体的完整执行追踪。它不仅对输出进行评分，还对过程本身进行评估。关键评估维度包括：

• 计划质量：计划是否逻辑结构合理且可行？

• 工具使用：是否选择了正确的工具并正确应用？

• 上下文处理：智能体是否有效使用了先验信息？

这种方法对过程评估特别有价值，失败通常源于有缺陷的中间步骤，而不是最终输出。

## **3.8 人在环（HITL）评估**

虽然自动化提供规模，但它在深度主观性和复杂领域知识方面很吃力。人在环（HITL）评估是捕获自动化系统错过的关键定性信号和细微判断的基本过程。

然而，我们必须摆脱人工评分提供完美"客观基本事实"的想法。对于高度主观的任务（如评估创造性质量或细微语气），完美的评分者间一致性是罕见的。相反，HITL是建立人类校准基准、确保智能体行为与复杂人类价值观、上下文需求和领域特定准确性对齐的不可或缺的方法论。

HITL过程涉及几个关键职能：

• 领域专业知识：对于专业智能体（如医疗、法律或金融），您必须利用领域专家来评估事实正确性和对特定行业标准的遵守。

• 解释细微差别：人类对于评判定义高质量交互的微妙品质至关重要，如语气、创造力、用户意图和复杂道德对齐。

• 创建"黄金集"：在自动化生效之前，人类必须建立"黄金标准"基准。这涉及策划一个全面的评估集，定义成功目标，并制作一套涵盖典型、边缘和对抗场景的稳健测试用例。

## **3.9 用户反馈与审阅者界面**

评估还必须捕获真实世界的用户反馈。每次交互都是有用的、清晰的和可信的信号。这种反馈包括定性信号（如赞/踩）和定量产品内成功指标，如编码智能体的拉取请求（PR）接受率或旅行智能体的成功预订完成率。最佳实践包括：

• 低摩擦反馈：赞/踩、快速滑块或简短评论。

• 上下文丰富的审阅：反馈应与完整的对话和智能体的推理追踪配对。

• 审阅者用户界面（UI）：双面板界面：左侧对话，右侧推理步骤，内联标记"坏计划"或"工具滥用"等问题。

• 治理仪表板：汇总反馈以突出显示反复出现的问题和风险。

没有可用的界面，评估框架在实践中就会失败。强大的UI使审阅者和用户的反馈可见、快速且可操作。

## **3.10 超越性能：负责任AI（RAI）与安全评估**

评估的最后一个维度不是作为组件，而是作为任何生产智能体的强制性、不可协商的门槛：负责任AI和安全性。一个100%有效但造成伤害的智能体是彻底的失败。

安全评估是一个必须贯穿整个开发生命周期的专业学科。这涉及：

• 系统性红队：积极尝试使用对抗场景来破坏智能体。这包括试图生成仇恨言论、泄露私人信息、传播有害刻板印象，或诱导智能体从事恶意行为。

• 自动化过滤器与人工审阅：实施技术过滤器以捕获策略违规行为，并将其与人工审阅相结合，因为单独的自动化可能无法捕获偏见或毒性的细微形式。

• 遵守准则：明确评估智能体的输出是否符合预定义的道德准则和原则，以确保对齐并防止意外后果。

最终，性能指标告诉我们智能体是否能完成工作，但安全评估告诉我们它是否应该完成。

## **3.11 总结与展望**

有效的智能体评估需要超越简单测试，转向战略性的层次框架。这种"由外向内"方法首先验证端到端任务完成（黑盒），然后在"玻璃盒"内分析完整轨迹——评估推理质量、工具使用、鲁棒性和效率。

评判这一过程需要一种混合方法：可扩展的自动化（如LLM作为评判者）与不可或缺的人机回环（HITL）评估者的细微判断相结合。该框架由负责任AI和安全评估的不可协商层保护，以构建可信赖的系统。

我们理解需要评判整个轨迹，但如果没有数据，这个框架纯粹是理论性的。为了实现这种"玻璃盒"评估，系统首先必须是可观测的。第3章将提供架构蓝图，从评估理论转向掌握可观测性的实践——通过掌握三大支柱：日志记录、追踪和指标。

# **4. 可观测性：洞察智能体的思维**

在第2章中，我们确立了AI智能体是一种新型软件。它们不仅仅是遵循指令；它们做决策。这种根本差异要求对质量保证采取新方法，将我们从传统软件监控带到可观测性的更深领域。

为了理解这种差异，让我们离开服务器机房，走进厨房。

## **4.1 从监控到真正的可观测性**

要掌握差异，让我们离开服务器机房，走进厨房。

## **4.2 厨房类比：流水线厨师与美食主厨**

传统软件是流水线厨师：想象一个快餐厨房。流水线厨师有一张制作汉堡的层压食谱卡。步骤是僵化和确定性的：烤面包30秒，烤饼90秒，加一片奶酪，两个泡菜，挤一团番茄酱。

• 这个世界的监控是一个清单。烤架温度对吗？厨师是否遵循了每一步？订单是否按时完成？我们正在验证一个已知的、可预测的过程。

AI智能体是"神秘盒"挑战中的美食主厨：主厨被赋予一个目标（"创造一道令人惊叹的甜点"）和一篮食材（用户的提示、数据和可用工具）。没有单一的正确食谱。他们可能会创造一个巧克力熔岩蛋糕、一个解构的提拉米苏或一个藏红花奶冻。所有都可能是有效的，甚至是绝妙的解决方案。

• 可观测性是美食评论家评判主厨的方式。评论家不仅仅品尝最终菜肴。他们想了解过程和推理。为什么主厨选择将覆盆子与罗勒搭配？他们使用了什么技术来结晶生姜？当意识到没有糖时他们是如何适应的？我们需要看到他们的"思维过程"内部，才能真正评估他们工作的质量。

这对AI智能体代表着根本转变，从简单监控到真正的可观测性。重点不再仅仅是验证智能体是否活跃，而是理解其认知过程的质量。关键问题不再是智能体在运行吗，而是智能体在有效思考吗。

## **4.3 可观测性的三大支柱**

那么，我们如何获得对智能体"思维过程"的访问权呢？我们不能直接读心，但我们可以分析它留下的证据。这是通过将我们的可观测性实践构建在三个基础支柱上来实现的：日志、追踪和指标。它们是将我们从品尝最终菜肴转变为评判整个烹饪表演的工具。

让我们剖析每个支柱，看看它们如何协同工作，为我们提供对智能体性能的评论者视角。

## **4.4 支柱1：日志记录——智能体的日记**

什么是日志？日志是可观测性的原子单位。把它们想象成智能体日记中的时间戳条目。每个条目是关于离散事件的原始、不可变的事实："在10:01:32，我被问了一个问题。在10:01:33，我决定使用get_weather工具。"它们告诉我们发生了什么。

超越print()：什么使日志有效？
像Google Cloud Logging这样的完全托管服务允许您大规模存储、搜索和分析日志数据。它可以自动从Google Cloud服务收集日志，其日志分析功能允许您运行SQL查询以发现智能体行为中的趋势。

一流的框架使这变得简单。例如，智能体开发工具包（ADK）基于Python的标准日志模块构建。这允许开发人员配置所需的详细程度——从生产中的高级INFO消息到开发过程中的粒度DEBUG消息——而无需更改智能体的代码。

关键日志条目的剖析
为了重构智能体的"思维过程"，日志必须具有丰富的上下文。结构化的JSON格式是黄金标准。

• 核心信息：好的日志捕获完整的上下文：提示/响应配对、中间推理步骤（智能体的"思维链"）、结构化工具调用（输入、输出、错误）以及对智能体内部状态的任何更改。

• 权衡：详细程度与性能：高度详细的DEBUG日志是开发人员排除故障的最佳朋友，但在生产环境中可能太"嘈杂"并产生性能开销。这就是为什么结构化日志如此强大；它允许您收集详细数据但有效过滤。

## **4.5 支柱2：追踪——跟随智能体的脚步**

什么是追踪？如果日志是日记条目，追踪就是将它们连接成连贯故事的叙事线索。追踪跟随单个任务——从初始用户查询到最终答案——将单个日志（称为跨度）拼接成完整的端到端视图。追踪通过显示事件之间的因果关系来揭示关键的"为什么"。

想象侦探的软木板。日志是个别线索——一张照片、一张票根。追踪是连接它们的红线，揭示事件的完整顺序。

## **4.6 为什么追踪不可或缺**

考虑一个复杂的智能体失败，用户问一个问题并得到无意义的答案。

• 孤立日志可能显示：错误：RAG搜索失败和错误：LLM响应验证失败。你看到了错误，但不清楚根本原因。

• 追踪揭示完整的因果链：用户查询 → RAG搜索（失败）→ 错误工具调用（收到空输入）→ LLM错误（对坏工具输出感到困惑）→ 错误最终答案

追踪使根本原因立即显而易见，使其对调试复杂的多步智能体行为不可或缺。

## **4.7 智能体追踪的关键要素**

现代追踪建立在OpenTelemetry等开放标准之上。核心组件是：

• 跨度：追踪中的单个命名操作（例如，llm_call跨度、tool_execution跨度）。

• 属性：附加到每个跨度的丰富元数据——prompt_id、latency_ms、token_count、user_id等。

• 上下文传播：通过唯一的trace_id连接跨度的"魔法"，允许Google Cloud Trace等后端组装完整的画面。Cloud Trace是一个分布式追踪系统，可帮助您了解应用程序处理请求需要多长时间。当智能体部署在Vertex AI Agent Engine等托管运行时上时，这种集成得到简化。Agent Engine处理在生产中扩展智能体的基础设施，并自动与Cloud Trace集成以提供端到端的可观测性，将智能体调用与所有后续的模型和工具调用联系起来。

## **4.8 支柱3：指标——智能体的健康报告**

什么是指标？如果日志是厨师的准备笔记，追踪是评论家逐步观看食谱展开的评论，那么指标就是评论家发布的最终记分卡。它们是定量的、聚合的健康分数，让您立即一目了然地了解智能体的整体性能。

至关重要的是，美食评论家不会仅仅基于对最终菜肴的单一品尝来发明这些分数。他们的判断基于他们观察到的一切。指标也是如此：它们不是新的数据源。它们是通过随时间聚合日志和追踪中的数据得出的。它们回答这个问题："平均而言，表现如何？"

对于AI智能体，将指标分为两个不同的类别很有用：可直接测量的系统指标和更复杂的评估性质量指标。

## **4.9 系统指标：生命体征**

系统指标是运营健康的基础定量衡量标准。它们直接通过对日志和追踪中的属性进行聚合函数（如平均值、总和或百分位数）计算得出。把这些想象成智能体的生命体征：脉搏、体温和血压。

需要跟踪的关键系统指标包括：

性能：
• 延迟（P50/P99）：通过聚合追踪中的duration_ms属性来找到中位数和第99百分位响应时间。这告诉您典型和最坏情况的用户体验。

• 错误率：包含error=true属性的追踪百分比。

成本：
• 每任务Token数：跨所有追踪的token_count属性的平均值，对管理LLM成本至关重要。

• 每次运行API成本：通过将token数与模型定价相结合，您可以跟踪每个任务的平均财务成本。

有效性：
• 任务完成率：成功达到指定"成功"跨度的追踪百分比。

• 工具使用频率：每个工具（如get_weather）作为跨度名称出现的次数计数，揭示哪些工具最有价值。

这些指标对运营、设置警报和管理智能体队列的成本和性能至关重要。

## **4.10 质量指标：评判决策制定**

质量指标是在原始可观测性数据之上应用第2章中详述的判断框架得出的二阶指标。它们超越效率来评估智能体的推理和最终输出质量本身。

这些不是简单的计数器或平均值。它们是通过在原始可观测性数据之上应用判断层得出的二阶指标。它们评估智能体的推理和最终输出的质量。

关键质量指标的示例包括：

• 正确性与准确性：智能体是否提供了事实正确的答案？如果它总结了文档，摘要是否忠实于来源？

• 轨迹遵守度：智能体是否遵循了给定任务的预期路径或"理想食谱"？它是否按正确顺序调用了正确的工具？

• 安全性与责任：智能体的回答是否避免了有害、偏见或不适当的内容？

• 有用性与相关性：智能体的最终回答对用户是否真正有用且与他们的查询相关？

生成这些指标需要的不只是简单的数据库查询。它通常涉及将智能体的输出与"黄金"数据集进行比较，或使用复杂的LLM作为评判者根据标准对响应进行评分。

来自日志和追踪的可观测数据是计算这些分数所需的基本证据，但判断过程本身是一个单独的、关键的学科。

## **4.11 整合一切：从原始数据到可操作的洞察**

拥有日志、追踪和指标就像拥有才华横溢的厨师、储备充足的储藏室和评判标准。但这些只是组件。要经营一家成功的餐厅，您需要将它们组装成一个适合繁忙晚餐服务的系统。本节是关于这种实际组装的——将您的可观测数据转化为实时操作和行动。

这涉及三个关键的运营实践：

1. 仪表板与警报：区分系统健康与模型质量
   单个仪表板是不够的。要有效管理AI智能体，您需要系统指标和质量指标的不同视图，因为它们服务于不同的目的和不同的团队。

• 运营仪表板（用于系统指标）：此类别仪表板专注于实时运营健康。它跟踪智能体的核心生命体征，主要面向负责系统正常运行时间和性能的站点可靠性工程师（SRE）、DevOps和运营团队。

- 跟踪内容：P99延迟、错误率、API成本、Token消耗。
- 目的：立即发现系统故障、性能下降或预算超支。
- 警报示例：警报：P99延迟> 3秒持续5分钟。这表示需要立即工程关注的系统瓶颈。

• 质量仪表板（用于质量指标）：此类别跟踪智能体有效性和正确性的更细微、变化较慢的指标。它对负责智能体决策和输出质量的产品所有者、数据科学家和AgentOps团队至关重要。

- 跟踪内容：事实正确性分数、轨迹遵守度、有用性评分、幻觉率。
- 目的：检测智能体质量的细微漂移，特别是在部署新模型或提示后。
- 警报示例：警报："有用性评分"在过去24小时内下降了10%。这表示虽然系统可能运行正常（系统指标正常），但智能体输出的质量正在下降，需要调查其逻辑或数据。

2. 安全性与PII：保护您的数据
   这是生产操作中不可协商的方面。日志和追踪中捕获的用户输入通常包含个人身份信息（PII）。强大的PII清理机制必须是日志管道的集成部分，然后再长期存储数据，以确保符合隐私法规并保护您的用户。
3. 核心权衡：粒度与开销
   在生产中为每个单个请求捕获高度详细的日志和追踪可能是 prohibitively expensive 的，并为您的系统增加延迟。关键是找到战略平衡。

• 最佳实践——动态采样：在开发环境中使用高粒度日志记录（DEBUG级别）。在生产中，设置较低的默认日志级别（INFO），但实施动态采样。例如，您可能决定只追踪10%的成功请求，但100%的所有错误。这为您提供了指标所需的广泛性能数据，而不会压垮您的系统，同时仍然捕获调试每次失败所需的丰富诊断细节。

## **4.12 总结与展望**

要信任自主智能体，您首先必须能够理解其过程。您不会在没有对美食主厨的食谱、技术和沿途决策有一定了解的情况下评判他们的最终菜肴。本章确立，可观测性是我们获得对智能体这种关键洞察的框架。它提供了厨房内的"眼睛和耳朵"。

我们了解到，强大的可观测性实践建立在三个基础支柱之上，它们协同工作将原始数据转化为完整的画面：

• 日志：结构化日记，提供每一步发生的事情的粒度事实记录。

• 追踪：连接单个日志的叙事故事，显示因果路径以揭示为什么会发生。

• 指标：聚合的成绩单，大规模总结性能以告诉我们事情发生得有多好。我们进一步将这些分为生命体征系统指标（如延迟和成本）和关键质量指标（如正确性和有用性）。

通过将这些支柱组装成一个连贯的运营系统，我们从盲目飞行转向对智能体行为、效率、有效性的清晰、数据驱动的视图。

我们现在有了所有的部分：为什么（第1章中非确定性问题）、什么（第2章的评估框架）和如何（第3章的可观测性架构）。

在第4章中，我们将把所有这些整合到一个运营手册中，展示这些组件如何形成"智能体质量飞轮"——一个持续改进的循环，以构建不仅有能力，而且真正可信赖的智能体。

# **5. 结论：在自主世界中建立信任**

在本白皮书的开头，我们提出了一个根本性的挑战：AI智能体以其非确定性和自主的性质，粉碎了我们传统的软件质量模型。我们将评估智能体的任务比作评估新员工——您不仅问任务是否完成，还问它是如何完成的。效率高吗？安全吗？体验好吗？当后果是业务风险时，盲目飞行不是一个选项。

自开篇以来的旅程是关于在这种新范式中建立信任蓝图的。我们通过定义智能体质量的四大支柱（有效性、成本效率、安全性和用户信任）来建立对新学科的需求。然后，我们展示了如何通过可观测性（第3章）获得智能体思维内的"眼睛和耳朵"，以及如何通过整体评估框架（第2章）评判其性能。本文奠定了衡量什么和如何看的基础。关键的下一步，在随后的白皮书"第5天：从原型到生产"中，是将这些原则操作化。这涉及通过强大的CI/CD管道、安全的推出策略和可扩展的基础设施，将评估过的智能体成功运行在生产环境中。

现在，我们把所有内容整合起来。这不仅仅是总结；它是将抽象原则转变为可靠、自我改进系统的运营手册，弥合评估与生产之间的差距。

## **5.1 引言：从自主能力到企业信任**

在本白皮书的开头，我们提出了一个根本性的挑战：AI智能体以其非确定性和自主的性质，粉碎了我们传统的软件质量模型。我们将评估智能体的任务比作评估新员工——您不仅问任务是否完成，还问它是如何完成的。效率高吗？安全吗？体验好吗？当后果是业务风险时，盲目飞行不是一个选项。

自开篇以来的旅程是关于在这种新范式中建立信任蓝图的。我们通过定义智能体质量的四大支柱（有效性、成本效率、安全性和用户信任）来建立对新学科的需求。然后，我们展示了如何通过可观测性（第3章）获得智能体思维内的"眼睛和耳朵"，以及如何通过整体评估框架（第2章）评判其性能。本文奠定了衡量什么和如何看的基础。关键的下一步，在随后的白皮书"第5天：从原型到生产"中，是将这些原则操作化。这涉及通过强大的CI/CD管道、安全的推出策略和可扩展的基础设施，将评估过的智能体成功运行在生产环境中。

现在，我们把所有内容整合起来。这不仅仅是总结；它是将抽象原则转变为可靠、自我改进系统的运营手册，弥合评估与生产之间的差距。

## **5.2 智能体质量飞轮：框架综合**

优秀的智能体不仅仅是执行；它改进。这种持续评估的学科将有能力的演示与企业级系统区分开来。这种实践创造了一种我们称之为智能体质量飞轮的强大、自我强化系统。

想象一下启动一个巨大、沉重的飞轮。第一次推动是最难的。但评估的结构化实践提供了后续一致的推动。每次推动都增加了动量，直到飞轮以不可阻挡的力量旋转，创造质量和信任的良性循环。这个飞轮是我们讨论的整个框架的运营体现。

以下是每个章节的组件如何协同工作以建立这种动量：

• 第1步：定义质量（目标）：飞轮需要一个方向。正如我们在第1章中定义的，一切都始于质量的四大支柱：有效性、成本效率、安全性和用户信任。这些支柱不是抽象的理想；它们是给我们的评估工作赋予意义并将飞轮与真正业务价值对齐的具体目标。

• 第2步：为可见性而检测（基础）：您无法管理您看不到的东西。如我们在可观测性章节中详述的，我们必须指示我们的智能体产生结构化日志（智能体的日记）和端到端追踪（叙事线索）。这种可观测性是生成衡量我们四大支柱所需丰富证据的基础实践，为飞轮提供必要的燃料。

• 第3步：评估过程（引擎）：有了可见性，我们现在可以评判性能。如我们的评估章节所探讨的，这涉及战略性的"由外向内"评估，评判最终输出和整个推理过程。这是推动飞轮旋转的强大推动力——一个混合引擎，使用可扩展的LLM作为评判者系统加快速度，使用人在环（HITL）"黄金标准"作为基本事实。

• 第4步：架构反馈循环（动量）：这就是第1章中的"可评估设计"架构的实现。通过构建关键的反馈循环，我们确保每次生产失败，当被捕获和注释时，都会被程序化地转换为我们的"黄金"评估集中的永久回归测试。每次失败都使系统更智能，使飞轮旋转得更快，推动持续、不懈的改进。

## **5.3 构建可信智能体的三大核心原则**

如果您从本白皮书中一无所获，请牢记这三个原则。它们代表了任何旨在在这种新的、智能体的最先进状态下构建真正可靠自主系统的领导者的基础心态。

• 原则1：将评估视为架构支柱，而不是最后一步：还记得第1章中的赛车类比吗？您不会先制造F1赛车然后加装传感器。您从一开始就设计它带有遥测端口。智能体工作负载需要相同的DevOps范式。可靠的智能体是"可评估设计"的，从第一行代码开始就进行检测以发出判断所需的日志和追踪。质量是架构选择，而不是最终的QA阶段。

• 原则2：轨迹即真理：对于智能体来说，最终答案只是长篇故事的最后一句话。正如我们在评估章节中确立的，智能体逻辑、安全性和效率的真正衡量标准在于其端到端"思维过程"——轨迹。这是过程评估。要真正理解智能体为什么成功或失败，您必须分析这条路径。这只有通过我们在第3章中详述的深度可观测性实践才可能实现。

• 原则3：人类是仲裁者：自动化是我们扩展规模的工具；人性是我们真理的源泉。从LLM作为评判者系统到安全分类器的自动化是必不可少的。然而，正如我们在人在环（HITL）评估的深入探讨中所确立的，"好"的基本定义、细微输出的验证以及对安全性和公平性的最终判断必须锚定在人类价值观上。AI可以帮助评分测试，但人类编写标准并决定A+的真正含义。

## **5.4 未来是智能体的——也是可靠的**

我们正处于智能体时代的黎明。创造能够推理、规划和行动的AI的能力将成为我们时代最具变革性的技术转变之一。但强大的力量带来了构建值得信任系统的深刻责任。

掌握本白皮书中的概念——可以称之为"评估工程"——是下一波AI的关键竞争优势。继续将智能体质量视为事后想法的组织将陷入有前途的演示和失败的部署的循环中。相比之下，那些投资于这种严格的、架构集成的评估方法的组织将超越炒作，部署真正变革性的、企业级AI系统。

最终目标不仅是构建能工作的智能体，而是构建值得信任的智能体。正如我们所展示的，这种信任不是希望或偶然的问题。它是在持续、全面和架构完善的评估熔炉中锻造的。

# **参考文献**

## **学术论文、书籍和正式报告**

1. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., ... & Rocktäschel, T. (2020). 检索增强生成用于知识密集型NLP任务. 神经信息处理系统进展, 33, 9459-9474.
2. Lin, S., Hilton, J., & Evans, O. (2022). TruthfulQA：衡量模型如何模仿人类虚假陈述. 第60届计算语言学协会年会论文集（第1卷：长论文）(pp. 3214–3252).
3. Li, D., Jiang, B., Huang, L., Beigi, A., Zhao, C., Tan, Z.,... & Liu, H. (2024). 从生成到判断：LLM作为评判者的机遇与挑战. arXiv预印本 arXiv:2411.16594.
4. Zhuge, M., Wang, M., Shen, X., Zhang, Y., Wang, Y., Zhang, C., ... & Liu, N. (2024). 智能体作为评判者：用智能体评估智能体. arXiv预印本 arXiv:2410.10934.
5. Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., & Mané, D. (2016). AI安全的具体问题. arXiv预印本 arXiv:1606.06565.
6. Baysan, M. S., Uysal, S., İşlek, İ., Çığ Karaman, Ç., & Güngör, T. (2025). LLM作为评判者：使用大型语言模型自动评估搜索查询解析. 大数据前沿, 8.
7. Felderer, M., & Ramler, R. (2021). 基于AI系统的质量保证：概述与挑战. 软件质量：云计算中软件工程和软件质量的复杂性与挑战 (pp. 38-51). Springer国际出版.
8. Hendrycks, D., Carlini, N., Schulman, J., & Steinhardt, J. (2023). ML安全中的未解决问题. arXiv预印本 arXiv:2306.04944.
9. Ji, Z., Lee, N., Fries, R., Yu, T., Su, D., Xu, Y.,... & Fung, P. (2023). AI生成文本：任务、评估标准和方法的调查. arXiv预印本 arXiv:2303.07233.
10. Lin, C. Y. (2004). ROUGE：自动评估摘要的软件包. ACL-04文本摘要分支研讨会论文集 (pp. 74-81).
11. 美国国家标准与技术研究院. (2023). AI风险管理框架（AI RMF 1.0）. 美国商务部.
12. Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. (2002). BLEU：机器翻译自动评估的方法. 第40届计算语言学协会年会论文集 (pp. 311-318).
13. Retzlaff, C., Das, S., Wayllace, C., Mousavi, P., Afshari, M., Yang, T., ... & Holzinger, A. (2024). 人在环强化学习：关于需求、挑战和机遇的调查与立场. 人工智能研究杂志, 79, 359-415.
14. Slattery, F., Costello, E., & Holland, J. (2024). 语言模型带来的风险分类法. arXiv预印本 arXiv:2401.12903.
15. Taylor, M. E. (2023). 强化学习需要人在环框架和方法. 2023年自适应与学习智能体（ALA）研讨会发表论文.
16. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E.,... & Zhou, D. (2022). 思维链提示引发大型语言模型中的推理. 神经信息处理系统进展, 35, 24824-24837.
17. Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., & Artzi, Y. (2020). BERTScore：使用BERT评估文本生成. 国际学习表征会议.

## **网络文章、博客文章和一般网页**

1. Bunnyshell. (n.d.). LLM作为评判者：AI如何更快、更智能地评估AI. 2025年9月16日检索，来自 https://www.bunnyshell.com/blog/when-ai-becomes-the-judge-understanding-llm-as-a-judge/
2. Coralogix. (n.d.). 用于AI的OpenTelemetry：追踪提示、工具和推理. 2025年9月16日检索，来自 https://coralogix.com/ai-blog/opentelemetry-for-ai-tracing-prompts-tools-and-inferences/
3. Drapkin, A. (2025, 9月2). AI出错：AI的错误、失误和幻觉（2023-2025）. Tech.co. 2025年9月16日检索，来自 https://tech.co/news/list-ai-failures-mistakes-errors
4. Dynatrace. (n.d.). 什么是OpenTelemetry？日志、指标和追踪的开放标准. 2025年9月16日检索，来自 https://www.dynatrace.com/news/blog/what-is-opentelemetry/
5. Galileo. (n.d.). LLM作为评判者评估综合指南. 2025年9月16日检索，来自 https://galileo.ai/blog/llm-as-a-judge-guide-evaluation
6. Gofast.ai. (n.d.). 现实世界中的智能体幻觉：当AI工具出错时. 2025年9月16日检索，来自 https://www.gofast.ai/blog/ai-bias-fairness-agent-hallucinations-validation-drift-2025
7. IBM. (2025, 2月25). 什么是LLM可观测性？2025年9月16日检索，来自 https://www.ibm.com/think/topics/llm-observability
8. MIT Sloan教学与学习技术. (n.d.). 当AI出错时：解决AI幻觉和偏见. 2025年9月16日检索，来自 https://mitsloanedtech.mit.edu/ai/basics/addressing-ai-hallucinations-and-bias/
9. ResearchGate. (n.d.). (PDF) LLM作为评判者调查. 2025年9月16日检索，来自 https://www.researchgate.net/publication/386112851_A_Survey_on_LLM-as-a-Judge
10. TrustArc. (n.d.). 美国国家标准与技术研究院（NIST）AI风险管理. 2025年9月16日检索，来自 https://trustarc.com/regulations/nist-ai-rmf/

---

本文档已翻译为中文，标题已添加序号
